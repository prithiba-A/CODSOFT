{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGg/RodUq0JXrMbI2OvtBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prithiba-A/CODSOFT/blob/main/Task_3_Image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybTEyE59AhL",
        "outputId": "4f4e917f-f441-4272-f9c8-9e08a9c75846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "# pip install torch torchvision nltk\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchvision import transforms\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Placeholder for loading image data and captions\n",
        "def load_data():\n",
        "    # Replace this with your actual data loading logic\n",
        "    # Return a tuple of (images, captions)\n",
        "    pass\n",
        "\n",
        "# Define the image preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define the encoder (VGG in this case)\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        vgg = models.vgg16(pretrained=True)\n",
        "        modules = list(vgg.children())[:-1]  # Remove the last fully connected layer\n",
        "        self.vgg = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(vgg.classifier[-1].in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.vgg(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "# Define the decoder (LSTM in this case)\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
        "        hiddens, _ = self.lstm(packed)\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Define the ImageCaptioning model\n",
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        self.encoder = EncoderCNN(embed_size)\n",
        "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions, lengths):\n",
        "        features = self.encoder(images)\n",
        "        features = features.unsqueeze(1)  # Add a sequence length dimension\n",
        "        outputs = self.decoder(features, captions, lengths)\n",
        "        return outputs\n"
      ]
    }
  ]
}